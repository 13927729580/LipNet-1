{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential # To initialise the nn as a sequence of layers\n",
    "from keras.layers import Convolution2D # To make the convolution layer for 2D images\n",
    "from keras.layers import MaxPooling2D # \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "csv=CSVLogger(\"1_rmsprop.log\")\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32,(2,2),input_shape = (224,224,1), activation = 'relu',strides=2,name='convo1'))\n",
    "classifier.add(Convolution2D(64,(3,3), activation = 'relu',name='convo2'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(64,(3,3),activation = 'relu',name='convo3'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Convolution2D(64,(3,3),activation = 'relu',name='convo4'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout((0.5)))\n",
    "classifier.add(Dense(1024, activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout((0.4)))\n",
    "classifier.add(Dense(20, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2600 images belonging to 20 classes.\n",
      "Found 200 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "curr_path = os.getcwd()\n",
    "basefolder = os.path.dirname(curr_path)\n",
    "train_folder = os.path.join(basefolder, \"Dataset\\\\Train\")\n",
    "test_folder = os.path.join(basefolder, \"Dataset\\\\dev\")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(train_folder,target_size=(224, 224),batch_size=32,class_mode='categorical',color_mode='grayscale')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_folder,target_size=(224, 224),batch_size=32,class_mode='categorical',color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 313s - loss: 0.3892 - acc: 0.8855 - val_loss: 4.2211 - val_acc: 0.3902\n",
      "Epoch 2/10\n",
      " - 306s - loss: 0.0360 - acc: 0.9881 - val_loss: 4.6059 - val_acc: 0.4441\n",
      "Epoch 3/10\n",
      " - 307s - loss: 0.0249 - acc: 0.9917 - val_loss: 4.4116 - val_acc: 0.4688\n",
      "Epoch 4/10\n",
      " - 307s - loss: 0.0201 - acc: 0.9937 - val_loss: 3.9990 - val_acc: 0.5052\n",
      "Epoch 5/10\n",
      " - 307s - loss: 0.0157 - acc: 0.9950 - val_loss: 3.5831 - val_acc: 0.4303\n",
      "Epoch 6/10\n",
      " - 306s - loss: 0.0155 - acc: 0.9951 - val_loss: 4.1874 - val_acc: 0.4953\n",
      "Epoch 7/10\n",
      " - 307s - loss: 0.0130 - acc: 0.9957 - val_loss: 4.2869 - val_acc: 0.5049\n",
      "Epoch 8/10\n",
      " - 308s - loss: 0.0116 - acc: 0.9961 - val_loss: 4.4034 - val_acc: 0.5246\n",
      "Epoch 9/10\n",
      " - 307s - loss: 0.0111 - acc: 0.9964 - val_loss: 5.5899 - val_acc: 0.4351\n",
      "Epoch 10/10\n",
      " - 307s - loss: 0.0101 - acc: 0.9969 - val_loss: 4.8772 - val_acc: 0.5307\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(train_set,steps_per_epoch=2600,epochs=10,validation_data=test_set,validation_steps=200,callbacks=[csv],verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('1_rmsprop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load_model(\"1_rmsprop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 307s - loss: 0.0107 - acc: 0.9968 - val_loss: 5.8916 - val_acc: 0.4703\n",
      "Epoch 2/10\n",
      " - 306s - loss: 0.0099 - acc: 0.9971 - val_loss: 5.5281 - val_acc: 0.4696\n",
      "Epoch 3/10\n",
      " - 307s - loss: 0.0103 - acc: 0.9968 - val_loss: 4.9190 - val_acc: 0.4895\n",
      "Epoch 4/10\n",
      " - 306s - loss: 0.0093 - acc: 0.9972 - val_loss: 5.1784 - val_acc: 0.5051\n",
      "Epoch 5/10\n",
      " - 307s - loss: 0.0091 - acc: 0.9974 - val_loss: 4.7376 - val_acc: 0.5044\n",
      "Epoch 6/10\n",
      " - 306s - loss: 0.0100 - acc: 0.9971 - val_loss: 5.4427 - val_acc: 0.5038\n",
      "Epoch 7/10\n",
      " - 306s - loss: 0.0109 - acc: 0.9970 - val_loss: 4.5272 - val_acc: 0.5196\n",
      "Epoch 8/10\n",
      " - 306s - loss: 0.0078 - acc: 0.9978 - val_loss: 5.1981 - val_acc: 0.4752\n",
      "Epoch 9/10\n",
      " - 306s - loss: 0.0087 - acc: 0.9975 - val_loss: 4.7845 - val_acc: 0.5089\n",
      "Epoch 10/10\n",
      " - 306s - loss: 0.0094 - acc: 0.9973 - val_loss: 5.6814 - val_acc: 0.4890\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(train_set,steps_per_epoch=2600,epochs=10,validation_data=test_set,validation_steps=200,callbacks=[csv],verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
